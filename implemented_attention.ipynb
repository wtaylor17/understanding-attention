{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implemented-attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO4tFqSLm-a7",
        "colab_type": "code",
        "outputId": "db023ddf-0f78-459c-e878-a4683431ca66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ganKGYcSLPS8",
        "colab_type": "text"
      },
      "source": [
        "# Encoder RNN (forward direction).\n",
        "\n",
        "# $\\mathbf{x} = (x_1, \\ldots, x_{T_x}) \\in \\mathcal{R}^{B \\times T_x \\times K_x}$ is the input sequence to the RNN (a batch of $B$ samples)\n",
        "# $\\mathbf{y} = (y_1, \\ldots, y_{T_y}) \\in \\mathcal{R}^{B \\times T_y \\times K_y}$ is the output sequence\n",
        "\n",
        "# The encoder produces states $\\overrightarrow{h}_t$ as follows\n",
        "\n",
        "# $$\\overrightarrow{h}_0 = 0, \\overrightarrow{h}_t = (1-\\overrightarrow{z}_t) \\circ \\overrightarrow{h}_{t-1} + \\overrightarrow{z}_t \\circ \\overrightarrow{\\underline{h}}_t$$\n",
        "# $$\\overrightarrow{\\underline{h}}_t = \\tanh\\left(x_i\\bar{E}\\overrightarrow{W} + \\left[\\overrightarrow{r}_t \\circ \\overrightarrow{h}_{t-1}\\right]U\\right)$$\n",
        "# $$\\overrightarrow{z}_t = \\sigma\\left(x_i\\bar{E}\\overrightarrow{W}_z + \\overrightarrow{h}_{t-1}\\overrightarrow{U}_z \\right)$$\n",
        "# $$\\overrightarrow{r}_t = \\sigma\\left(x_i \\bar{E} \\overrightarrow{W}_r + \\overrightarrow{h}_{t-1}\\overrightarrow{U}_r \\right)$$\n",
        "\n",
        "# It may be helpful to read the following equations in reverse over (starting with $r$). Here, $\\bar{E}$ is the embedding matrix for inputs.\n",
        "# We define a class `MultiInputDense` to compute $z, r$, and $\\underline{h}$, before wrapping them in a `EncoderCell` class to use for RNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fAtOgLdRnOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiInputDense(keras.layers.Layer):\n",
        "    def __init__(self, units, n_inputs=2, activation='linear', use_bias=False, **kwargs):\n",
        "        self.dense_layers = [keras.layers.Dense(units,\n",
        "                                                use_bias=use_bias,\n",
        "                                                activation=activation)\n",
        "                              for _ in range(n_inputs)]\n",
        "        self.units = units\n",
        "        self.activation = keras.layers.Activation(activation)\n",
        "        super(MultiInputDense, self).__init__(**kwargs)\n",
        "    \n",
        "    def build_output_shape(input_shape):\n",
        "        return input_shape[:-1] + (self.units,)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        layer_outputs = [layer(x) \n",
        "                        for (layer, x) in zip(self.dense_layers, inputs)]\n",
        "        return tf.math.add_n(layer_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG2G-MYHTEaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderCell(keras.layers.Layer):\n",
        "    def __init__(self, units, embedding_dim=128, **kwargs):\n",
        "        self.r_dense = MultiInputDense(units, activation='sigmoid')\n",
        "        self.z_dense = MultiInputDense(units, activation='sigmoid')\n",
        "        self.h_dense = MultiInputDense(units, activation='tanh')\n",
        "        self.embedding = keras.layers.Dense(embedding_dim,\n",
        "                                            activation='linear',\n",
        "                                            use_bias=False)\n",
        "        self.state_size = units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.units = units\n",
        "        super(EncoderCell, self).__init__(**kwargs)\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[1]\n",
        "    \n",
        "    def call(self, x, states): # [state_0, .., prev_state]\n",
        "        x = self.embedding(x)\n",
        "        h_prev = states[0]\n",
        "        r = self.r_dense([x, h_prev])\n",
        "        z = self.z_dense([x, h_prev])\n",
        "        hbar = self.h_dense([x, r * h_prev])\n",
        "        h = (1 - z) * h_prev + z * hbar\n",
        "        return h, [h]\n",
        "    \n",
        "    def get_config(self):\n",
        "        return {'units': self.units, 'embedding_dim': self.embedding_dim}\n",
        "\n",
        "def AttentionEncoder(units, embedding_dim=128, **kwargs):\n",
        "    from keras.utils.generic_utils import CustomObjectScope\n",
        "    with CustomObjectScope({'EncoderCell': EncoderCell}):\n",
        "        return keras.layers.Bidirectional(\n",
        "                    keras.layers.RNN(EncoderCell(units, embedding_dim=embedding_dim, **kwargs), return_sequences=True)\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq6Yp68gHqap",
        "colab_type": "text"
      },
      "source": [
        "# Decoder RNN\n",
        "# $$s_0 = \\tanh{(W_s \\overleftarrow{h}_1)}\\\\s_i = (1 - z_i) \\circ s_{i-1} + z_i \\circ \\tilde{s}_i$$\n",
        "# $$\\tilde{s}_i = \\tanh{(WEy_{i-1} + U[r_i \\circ s_{i-1}] + Cc_i)}$$\n",
        "# $$z_i = \\sigma\\left(W_z E y_{i-1} + U_z s_{i-1} + C_z c_i\\right)$$\n",
        "# $$r_i = \\sigma\\left(W_r E y_{i-1} + U_r s_{i-1} + C_r c_i\\right)$$\n",
        "# $$c_i = \\sum_{j}{\\alpha_{ij}h_j} \\in \\mathcal{R}^{m \\times 2n}$$\n",
        "# $$\\alpha_{ij} = \\frac{\\exp{e_{ij}}}{\\sum_{k}{\\exp{e_{ik}}}} = \\text{softmax}(e_{i:})_j$$\n",
        "# $$e_{ij} = v_a^\\top\\tanh{(W_a s_{i-1} + U_a h_j)}$$\n",
        "\n",
        "# First things first, we need $e_{ij}$. First notice that $U_a h_j$ is needed $T$ times for each $j$. Thus to save time, we will compute the tensor $P_{ijk} = (U_a h_j)_{ik}$. We call this the `StateScoreSubLayer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW5O-sCuJOf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def StateScoreSubLayer(units):\n",
        "    return keras.layers.TimeDistributed(keras.layers.Dense(units,\n",
        "                                                          activation=None,\n",
        "                                                          use_bias=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1aiVjJ71dbd",
        "colab_type": "text"
      },
      "source": [
        "# Next we will write a layer to compute $(e_{i1}, \\ldots, e_{iT})$ when given $s_{i-1}$ and $P$. We call this layer `SingleAttentionScore`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dBwj2Ym1zov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SingleAttentionScore(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(SingleAttentionScore, self).__init__(**kwargs)\n",
        "        self.dense_W = keras.layers.Dense(units,\n",
        "                                          activation=None,\n",
        "                                          use_bias=False)\n",
        "        self.dense_v = StateScoreSubLayer(1)\n",
        "        self.reshaper = keras.layers.Lambda(\n",
        "            lambda x: x[:, :, 0] # is (?, ?, 1) but want (?, ?)\n",
        "        )\n",
        "        self.activation = keras.layers.Activation('tanh')\n",
        "    \n",
        "    def compute_output_shape(self, input_shape, **kwargs):\n",
        "        state_shape, P_shape = input_shape\n",
        "        return (P_shape[0], P_shape[1])\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        prev_decoder_state, P = inputs\n",
        "        ws = self.dense_W(prev_decoder_state)\n",
        "        # we can broadcast ws to add to P\n",
        "        ws = tf.expand_dims(ws, 1)\n",
        "        tanh_input = self.activation(ws + P)\n",
        "        return self.reshaper(self.dense_v(tanh_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLLk-4EU2Oc7",
        "colab_type": "text"
      },
      "source": [
        "# Now we are ready to compute $c_i$. The layer below, `AttentionDecoderInputs`, will compute $c_i$ when given the tensors $s_{i-1}, H_{ijk} = (h_j)_{ik}, P$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYwE5kfl2MuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoderInputs(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(AttentionDecoderInputs, self).__init__(**kwargs)\n",
        "        self.sa_score = SingleAttentionScore(units)\n",
        "        self.softmax = keras.layers.Activation('softmax')\n",
        "        self.units = units\n",
        "    \n",
        "    def compute_output_shape(self, input_shape, **kwargs):\n",
        "        state_shape, H_shape, P_shape = input_shape\n",
        "        return (H_shape[0], H_shape[-1])\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        prev_state, H, P = inputs\n",
        "        A = self.sa_score([prev_state, P])\n",
        "        alpha = self.softmax(A)\n",
        "        alpha = tf.expand_dims(alpha, 2)\n",
        "        c_nosum = alpha * H\n",
        "        return keras.backend.sum(c_nosum, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZKfrEdb55aM",
        "colab_type": "text"
      },
      "source": [
        "# Now we will create an layer that encompasses the decoder. The layer will take in $H$ and give back $\\mathbf{y}$.\n",
        "\n",
        "# The paper assumes that\n",
        "# $$p(y_i|s_i, y_{i-1}, c_i) \\propto \\exp{(y_i^\\top W_o t_i)}$$\n",
        "# Get back to this equation in a minute. First,\n",
        "# $$t_i = [\\max{\\{\\tilde{t}_{i, 2j-1}, \\tilde{t}_{i, 2j}\\}}]^\\top_{j=1,\\ldots,l}$$\n",
        "# Where\n",
        "# $$\\tilde{t}_i = U_o s_{i-1} + V_o E y_{i-1} + C_o c_i \\in \\mathcal{R}^{2l}$$\n",
        "# Consider the vector $W_o t_i \\in \\mathcal{R}^{K_y}$ Since $y_i$ is some one-hot encoded vector, we have that if $y_i$ is representing the $j$th output class then $y_i^\\top W_o t_i = (W_o t_i)_j$. Thus, we can rewrite the equation as:\n",
        "\n",
        "# $$p(\\text{class }j|s_i, y_{i-1}, c_i) \\propto \\exp{(W_o t_i)}_j$$\n",
        "# which means that ultimately we take $\\text{softmax}(W_o t_i)$ as our output probability vector, and can generate $y_i$ as a one-hot version of the most probable class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgrKlye7lV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttentionDecoder(keras.layers.Layer):\n",
        "    def __init__(self, units, output_size, embedding_dim=128, latent_dim=64, return_sequences=False, **kwargs):\n",
        "        self.decoder_input_builder = AttentionDecoderInputs(units)\n",
        "        self.ss_sl = StateScoreSubLayer(units)\n",
        "        self.embedding = keras.layers.Dense(embedding_dim,\n",
        "                                            activation='linear',\n",
        "                                            use_bias=False)\n",
        "        self.z_gate = MultiInputDense(units, n_inputs=3, activation='sigmoid')\n",
        "        self.r_gate = MultiInputDense(units, n_inputs=3, activation='sigmoid')\n",
        "        self.s_gate = MultiInputDense(units, n_inputs=3, activation='tanh')\n",
        "        self.t_gate = MultiInputDense(2*latent_dim, n_inputs=3)\n",
        "        self.initial_state_gate = keras.layers.Dense(units,\n",
        "                                                     activation='tanh',\n",
        "                                                     use_bias=False)\n",
        "        self.y_gate = keras.layers.Dense(output_size, activation='softmax', use_bias=False)\n",
        "        self.output_size = output_size\n",
        "        self.units = units\n",
        "        self.latent_dim = latent_dim\n",
        "        self.return_sequences = return_sequences\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "    \n",
        "    def compute_output_shape(self, input_shape, **kwargs):\n",
        "        output_shape = (input_shape[0],)\n",
        "        if self.return_sequences:\n",
        "            output_shape += (input_shape[1],)\n",
        "        return output_shape + (self.output_size,)\n",
        "    \n",
        "    def call(self, H):\n",
        "        sequence_length = keras.backend.int_shape(H)[-2]\n",
        "        decoder_states = [self.initial_state_gate(H[:, 0, self.units:])]\n",
        "        decoder_outputs = []\n",
        "        P = self.ss_sl(H)\n",
        "        for t in range(sequence_length):\n",
        "            c = self.decoder_input_builder([decoder_states[-1], H, P])\n",
        "            # order is c, s, y always\n",
        "            inputs = [c, decoder_states[-1]]\n",
        "            additional_inputs = []\n",
        "            if t > 0:\n",
        "                additional_inputs = [self.embedding(decoder_outputs[-1])]\n",
        "            inputs += additional_inputs\n",
        "            r = self.r_gate(inputs)\n",
        "            z = self.r_gate(inputs)\n",
        "            rs = r * decoder_states[-1]\n",
        "            inputs = [c, rs]\n",
        "            inputs += additional_inputs\n",
        "            s_tilde = self.s_gate(inputs)\n",
        "            new_state = (1 - z) * decoder_states[-1] + z * s_tilde\n",
        "            inputs = [c, decoder_states[-1]]\n",
        "            inputs += additional_inputs\n",
        "            t_tilde = self.t_gate(inputs)\n",
        "            t_maximums = [keras.backend.max(t_tilde[:, 2*j:2*j+2], axis=1) for j in range(self.latent_dim)]\n",
        "            t = tf.stack(t_maximums, axis=1)\n",
        "            probs = self.y_gate(t)\n",
        "            decoder_states.append(new_state)\n",
        "            decoder_outputs.append(probs)\n",
        "        return tf.stack(decoder_outputs, axis=1) if self.return_sequences else decoder_outputs[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_-z_hoaZ3m",
        "colab_type": "text"
      },
      "source": [
        "# Finally a function to just build a `keras.models.Model` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_97HlxqFaUfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AttentionRNN(input_shape, units, n_classes, embedding_dim=64, latent_dim=16, return_sequences=False):\n",
        "    sequence_len, input_dim = input_shape\n",
        "    inputs = keras.layers.Input(input_shape)\n",
        "    encoder = AttentionEncoder(units, embedding_dim=embedding_dim)\n",
        "    decoder = AttentionDecoder(units, n_classes, embedding_dim=embedding_dim,\n",
        "                               latent_dim=latent_dim, return_sequences=return_sequences)\n",
        "    model = keras.models.Model(inputs, decoder(encoder(inputs)))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  metrics=['acc'],\n",
        "                  optimizer='adam')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MQaJTJDmjBF",
        "colab_type": "text"
      },
      "source": [
        "# Pet learning task (my implementation is for learning purposes not applications): class of $y$ is the square of the class of $x$, modulo 12 (the number of classes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKf5jGQ7FWPt",
        "colab_type": "code",
        "outputId": "d062ca51-67f3-4d1b-910d-11f3da96e944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.randint(0, high=12, size=(100, 10))\n",
        "y = ((x*x) % 12)[:, 0]\n",
        "x = keras.utils.to_categorical(x)\n",
        "y = keras.utils.to_categorical(y)\n",
        "\n",
        "mdl = AttentionRNN(x.shape[1:], 16, y.shape[1])\n",
        "mdl.summary()\n",
        "history = mdl.fit(x, y, epochs=300).history"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 10, 12)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 10, 32)            768       \n",
            "_________________________________________________________________\n",
            "attention_decoder_2 (Attenti (None, 10)                1840      \n",
            "=================================================================\n",
            "Total params: 2,608\n",
            "Trainable params: 2,608\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 3.3322 - acc: 0.0800\n",
            "Epoch 2/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 3.0914 - acc: 0.0900\n",
            "Epoch 3/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.9120 - acc: 0.1000\n",
            "Epoch 4/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.7350 - acc: 0.1300\n",
            "Epoch 5/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.6031 - acc: 0.1500\n",
            "Epoch 6/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.4980 - acc: 0.1800\n",
            "Epoch 7/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.4079 - acc: 0.2200\n",
            "Epoch 8/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.3108 - acc: 0.2100\n",
            "Epoch 9/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.2202 - acc: 0.2200\n",
            "Epoch 10/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.1290 - acc: 0.2700\n",
            "Epoch 11/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 2.0526 - acc: 0.2800\n",
            "Epoch 12/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.9842 - acc: 0.2900\n",
            "Epoch 13/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.9193 - acc: 0.2900\n",
            "Epoch 14/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.8589 - acc: 0.3500\n",
            "Epoch 15/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.8072 - acc: 0.3900\n",
            "Epoch 16/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.7546 - acc: 0.4300\n",
            "Epoch 17/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.7061 - acc: 0.4400\n",
            "Epoch 18/300\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.6605 - acc: 0.4400\n",
            "Epoch 19/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.6125 - acc: 0.4500\n",
            "Epoch 20/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.5588 - acc: 0.4800\n",
            "Epoch 21/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.5144 - acc: 0.5000\n",
            "Epoch 22/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.4830 - acc: 0.5100\n",
            "Epoch 23/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.4472 - acc: 0.5400\n",
            "Epoch 24/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.4189 - acc: 0.5600\n",
            "Epoch 25/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.3879 - acc: 0.5900\n",
            "Epoch 26/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.3607 - acc: 0.5600\n",
            "Epoch 27/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.3335 - acc: 0.5700\n",
            "Epoch 28/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.3140 - acc: 0.5900\n",
            "Epoch 29/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.2884 - acc: 0.5900\n",
            "Epoch 30/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.2619 - acc: 0.5900\n",
            "Epoch 31/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.2387 - acc: 0.6100\n",
            "Epoch 32/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.2184 - acc: 0.6100\n",
            "Epoch 33/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.1978 - acc: 0.6000\n",
            "Epoch 34/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.1775 - acc: 0.6300\n",
            "Epoch 35/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.1554 - acc: 0.6400\n",
            "Epoch 36/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.1356 - acc: 0.6600\n",
            "Epoch 37/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.1187 - acc: 0.6500\n",
            "Epoch 38/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.1035 - acc: 0.6900\n",
            "Epoch 39/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.0885 - acc: 0.7000\n",
            "Epoch 40/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.0808 - acc: 0.6900\n",
            "Epoch 41/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.0550 - acc: 0.7000\n",
            "Epoch 42/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.0413 - acc: 0.6800\n",
            "Epoch 43/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.0273 - acc: 0.6800\n",
            "Epoch 44/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 1.0047 - acc: 0.6800\n",
            "Epoch 45/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9851 - acc: 0.6900\n",
            "Epoch 46/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9706 - acc: 0.7000\n",
            "Epoch 47/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9535 - acc: 0.7000\n",
            "Epoch 48/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9415 - acc: 0.7100\n",
            "Epoch 49/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9316 - acc: 0.6900\n",
            "Epoch 50/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9197 - acc: 0.7000\n",
            "Epoch 51/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9178 - acc: 0.6900\n",
            "Epoch 52/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9226 - acc: 0.6800\n",
            "Epoch 53/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.9044 - acc: 0.7000\n",
            "Epoch 54/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8930 - acc: 0.7200\n",
            "Epoch 55/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8846 - acc: 0.7500\n",
            "Epoch 56/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8620 - acc: 0.7700\n",
            "Epoch 57/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8673 - acc: 0.7400\n",
            "Epoch 58/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8482 - acc: 0.7700\n",
            "Epoch 59/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8339 - acc: 0.7600\n",
            "Epoch 60/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8257 - acc: 0.7700\n",
            "Epoch 61/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8149 - acc: 0.7700\n",
            "Epoch 62/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.8058 - acc: 0.7600\n",
            "Epoch 63/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7914 - acc: 0.7600\n",
            "Epoch 64/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7865 - acc: 0.7600\n",
            "Epoch 65/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7752 - acc: 0.7700\n",
            "Epoch 66/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7661 - acc: 0.7900\n",
            "Epoch 67/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7572 - acc: 0.7900\n",
            "Epoch 68/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7453 - acc: 0.8000\n",
            "Epoch 69/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7351 - acc: 0.8100\n",
            "Epoch 70/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7223 - acc: 0.8200\n",
            "Epoch 71/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7120 - acc: 0.8400\n",
            "Epoch 72/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7041 - acc: 0.8400\n",
            "Epoch 73/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6960 - acc: 0.8200\n",
            "Epoch 74/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6921 - acc: 0.8400\n",
            "Epoch 75/300\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.6843 - acc: 0.8400\n",
            "Epoch 76/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6808 - acc: 0.8300\n",
            "Epoch 77/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6925 - acc: 0.8300\n",
            "Epoch 78/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.7152 - acc: 0.8200\n",
            "Epoch 79/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6889 - acc: 0.8200\n",
            "Epoch 80/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6833 - acc: 0.7900\n",
            "Epoch 81/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6757 - acc: 0.8000\n",
            "Epoch 82/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6612 - acc: 0.8300\n",
            "Epoch 83/300\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.6506 - acc: 0.8500\n",
            "Epoch 84/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6423 - acc: 0.8500\n",
            "Epoch 85/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6348 - acc: 0.8700\n",
            "Epoch 86/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6407 - acc: 0.8600\n",
            "Epoch 87/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6237 - acc: 0.8700\n",
            "Epoch 88/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6204 - acc: 0.8700\n",
            "Epoch 89/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6125 - acc: 0.8700\n",
            "Epoch 90/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6088 - acc: 0.8700\n",
            "Epoch 91/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6033 - acc: 0.8800\n",
            "Epoch 92/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5993 - acc: 0.8800\n",
            "Epoch 93/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5890 - acc: 0.8800\n",
            "Epoch 94/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5841 - acc: 0.8900\n",
            "Epoch 95/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5777 - acc: 0.8900\n",
            "Epoch 96/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5754 - acc: 0.8900\n",
            "Epoch 97/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5676 - acc: 0.8900\n",
            "Epoch 98/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5617 - acc: 0.9000\n",
            "Epoch 99/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5520 - acc: 0.9000\n",
            "Epoch 100/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5481 - acc: 0.9000\n",
            "Epoch 101/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5425 - acc: 0.9000\n",
            "Epoch 102/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5348 - acc: 0.9000\n",
            "Epoch 103/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5228 - acc: 0.9100\n",
            "Epoch 104/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5286 - acc: 0.9100\n",
            "Epoch 105/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5418 - acc: 0.9000\n",
            "Epoch 106/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5313 - acc: 0.9100\n",
            "Epoch 107/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5186 - acc: 0.9200\n",
            "Epoch 108/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.5057 - acc: 0.9300\n",
            "Epoch 109/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4912 - acc: 0.9200\n",
            "Epoch 110/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4831 - acc: 0.9100\n",
            "Epoch 111/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4704 - acc: 0.9200\n",
            "Epoch 112/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4648 - acc: 0.9300\n",
            "Epoch 113/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4586 - acc: 0.9200\n",
            "Epoch 114/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4516 - acc: 0.9300\n",
            "Epoch 115/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4453 - acc: 0.9300\n",
            "Epoch 116/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4407 - acc: 0.9300\n",
            "Epoch 117/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4352 - acc: 0.9300\n",
            "Epoch 118/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4298 - acc: 0.9400\n",
            "Epoch 119/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4238 - acc: 0.9500\n",
            "Epoch 120/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4200 - acc: 0.9400\n",
            "Epoch 121/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4152 - acc: 0.9400\n",
            "Epoch 122/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4135 - acc: 0.9300\n",
            "Epoch 123/300\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.4095 - acc: 0.9400\n",
            "Epoch 124/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4035 - acc: 0.9400\n",
            "Epoch 125/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3968 - acc: 0.9500\n",
            "Epoch 126/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3924 - acc: 0.9500\n",
            "Epoch 127/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3874 - acc: 0.9400\n",
            "Epoch 128/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3851 - acc: 0.9500\n",
            "Epoch 129/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3822 - acc: 0.9500\n",
            "Epoch 130/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3779 - acc: 0.9500\n",
            "Epoch 131/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3717 - acc: 0.9500\n",
            "Epoch 132/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3685 - acc: 0.9600\n",
            "Epoch 133/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3652 - acc: 0.9600\n",
            "Epoch 134/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3616 - acc: 0.9600\n",
            "Epoch 135/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3583 - acc: 0.9600\n",
            "Epoch 136/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3546 - acc: 0.9600\n",
            "Epoch 137/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3514 - acc: 0.9600\n",
            "Epoch 138/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3502 - acc: 0.9600\n",
            "Epoch 139/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3457 - acc: 0.9600\n",
            "Epoch 140/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3444 - acc: 0.9600\n",
            "Epoch 141/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3408 - acc: 0.9600\n",
            "Epoch 142/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3392 - acc: 0.9600\n",
            "Epoch 143/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3337 - acc: 0.9600\n",
            "Epoch 144/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3334 - acc: 0.9600\n",
            "Epoch 145/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3287 - acc: 0.9700\n",
            "Epoch 146/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3266 - acc: 0.9700\n",
            "Epoch 147/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3236 - acc: 0.9700\n",
            "Epoch 148/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3214 - acc: 0.9700\n",
            "Epoch 149/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3187 - acc: 0.9800\n",
            "Epoch 150/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3163 - acc: 0.9800\n",
            "Epoch 151/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3127 - acc: 0.9800\n",
            "Epoch 152/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3095 - acc: 0.9800\n",
            "Epoch 153/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3097 - acc: 0.9800\n",
            "Epoch 154/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3028 - acc: 0.9800\n",
            "Epoch 155/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3003 - acc: 0.9800\n",
            "Epoch 156/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2966 - acc: 0.9800\n",
            "Epoch 157/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2960 - acc: 0.9800\n",
            "Epoch 158/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2941 - acc: 0.9800\n",
            "Epoch 159/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2946 - acc: 0.9800\n",
            "Epoch 160/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2903 - acc: 0.9700\n",
            "Epoch 161/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3090 - acc: 0.9700\n",
            "Epoch 162/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3128 - acc: 0.9700\n",
            "Epoch 163/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3128 - acc: 0.9700\n",
            "Epoch 164/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3067 - acc: 0.9700\n",
            "Epoch 165/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3098 - acc: 0.9600\n",
            "Epoch 166/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2841 - acc: 0.9800\n",
            "Epoch 167/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2806 - acc: 0.9800\n",
            "Epoch 168/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2758 - acc: 0.9800\n",
            "Epoch 169/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2689 - acc: 0.9800\n",
            "Epoch 170/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2631 - acc: 0.9800\n",
            "Epoch 171/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2598 - acc: 0.9800\n",
            "Epoch 172/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2574 - acc: 0.9800\n",
            "Epoch 173/300\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.2554 - acc: 0.9800\n",
            "Epoch 174/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2512 - acc: 0.9800\n",
            "Epoch 175/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2488 - acc: 0.9800\n",
            "Epoch 176/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2458 - acc: 0.9800\n",
            "Epoch 177/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2435 - acc: 0.9800\n",
            "Epoch 178/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2411 - acc: 0.9800\n",
            "Epoch 179/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2396 - acc: 0.9800\n",
            "Epoch 180/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2373 - acc: 0.9800\n",
            "Epoch 181/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2348 - acc: 0.9800\n",
            "Epoch 182/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2341 - acc: 0.9800\n",
            "Epoch 183/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2330 - acc: 0.9800\n",
            "Epoch 184/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2464 - acc: 0.9700\n",
            "Epoch 185/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2385 - acc: 0.9700\n",
            "Epoch 186/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2294 - acc: 0.9800\n",
            "Epoch 187/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2299 - acc: 0.9800\n",
            "Epoch 188/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2266 - acc: 0.9800\n",
            "Epoch 189/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2225 - acc: 0.9800\n",
            "Epoch 190/300\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.2246 - acc: 0.9800\n",
            "Epoch 191/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2308 - acc: 0.9800\n",
            "Epoch 192/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2207 - acc: 0.9800\n",
            "Epoch 193/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2207 - acc: 0.9800\n",
            "Epoch 194/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2157 - acc: 0.9800\n",
            "Epoch 195/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2129 - acc: 0.9800\n",
            "Epoch 196/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2221 - acc: 0.9700\n",
            "Epoch 197/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2152 - acc: 0.9700\n",
            "Epoch 198/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2065 - acc: 0.9800\n",
            "Epoch 199/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2028 - acc: 0.9800\n",
            "Epoch 200/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2016 - acc: 0.9800\n",
            "Epoch 201/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1989 - acc: 0.9800\n",
            "Epoch 202/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1963 - acc: 0.9800\n",
            "Epoch 203/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1938 - acc: 0.9800\n",
            "Epoch 204/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1912 - acc: 0.9800\n",
            "Epoch 205/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1891 - acc: 0.9800\n",
            "Epoch 206/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1877 - acc: 0.9800\n",
            "Epoch 207/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1871 - acc: 0.9800\n",
            "Epoch 208/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1856 - acc: 0.9800\n",
            "Epoch 209/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1826 - acc: 0.9800\n",
            "Epoch 210/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1794 - acc: 0.9800\n",
            "Epoch 211/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1780 - acc: 0.9800\n",
            "Epoch 212/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1758 - acc: 0.9800\n",
            "Epoch 213/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1737 - acc: 0.9800\n",
            "Epoch 214/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1725 - acc: 0.9800\n",
            "Epoch 215/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1709 - acc: 0.9800\n",
            "Epoch 216/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1697 - acc: 0.9800\n",
            "Epoch 217/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1678 - acc: 0.9800\n",
            "Epoch 218/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1664 - acc: 0.9800\n",
            "Epoch 219/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1651 - acc: 0.9800\n",
            "Epoch 220/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1638 - acc: 0.9800\n",
            "Epoch 221/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1622 - acc: 0.9800\n",
            "Epoch 222/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1610 - acc: 0.9900\n",
            "Epoch 223/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1593 - acc: 0.9900\n",
            "Epoch 224/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1582 - acc: 0.9900\n",
            "Epoch 225/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1568 - acc: 0.9900\n",
            "Epoch 226/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1552 - acc: 0.9900\n",
            "Epoch 227/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1543 - acc: 0.9900\n",
            "Epoch 228/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1528 - acc: 0.9900\n",
            "Epoch 229/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1517 - acc: 0.9900\n",
            "Epoch 230/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1504 - acc: 0.9900\n",
            "Epoch 231/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1484 - acc: 0.9900\n",
            "Epoch 232/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1475 - acc: 0.9900\n",
            "Epoch 233/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1459 - acc: 0.9900\n",
            "Epoch 234/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1445 - acc: 0.9900\n",
            "Epoch 235/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1428 - acc: 0.9900\n",
            "Epoch 236/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1414 - acc: 0.9900\n",
            "Epoch 237/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1405 - acc: 0.9900\n",
            "Epoch 238/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1390 - acc: 0.9900\n",
            "Epoch 239/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1376 - acc: 0.9900\n",
            "Epoch 240/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1362 - acc: 0.9900\n",
            "Epoch 241/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1353 - acc: 0.9900\n",
            "Epoch 242/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1340 - acc: 0.9900\n",
            "Epoch 243/300\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.1329 - acc: 0.9900\n",
            "Epoch 244/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1317 - acc: 0.9900\n",
            "Epoch 245/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1305 - acc: 1.0000\n",
            "Epoch 246/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1296 - acc: 1.0000\n",
            "Epoch 247/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1284 - acc: 1.0000\n",
            "Epoch 248/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1273 - acc: 1.0000\n",
            "Epoch 249/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1263 - acc: 1.0000\n",
            "Epoch 250/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1257 - acc: 1.0000\n",
            "Epoch 251/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1241 - acc: 1.0000\n",
            "Epoch 252/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1230 - acc: 1.0000\n",
            "Epoch 253/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1212 - acc: 1.0000\n",
            "Epoch 254/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1205 - acc: 1.0000\n",
            "Epoch 255/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1196 - acc: 1.0000\n",
            "Epoch 256/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1185 - acc: 1.0000\n",
            "Epoch 257/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1174 - acc: 1.0000\n",
            "Epoch 258/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1167 - acc: 1.0000\n",
            "Epoch 259/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1154 - acc: 1.0000\n",
            "Epoch 260/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1137 - acc: 1.0000\n",
            "Epoch 261/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1126 - acc: 1.0000\n",
            "Epoch 262/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1113 - acc: 1.0000\n",
            "Epoch 263/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1103 - acc: 1.0000\n",
            "Epoch 264/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1091 - acc: 1.0000\n",
            "Epoch 265/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1083 - acc: 1.0000\n",
            "Epoch 266/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1073 - acc: 1.0000\n",
            "Epoch 267/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1064 - acc: 1.0000\n",
            "Epoch 268/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1057 - acc: 1.0000\n",
            "Epoch 269/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1047 - acc: 1.0000\n",
            "Epoch 270/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1040 - acc: 1.0000\n",
            "Epoch 271/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1031 - acc: 1.0000\n",
            "Epoch 272/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1020 - acc: 1.0000\n",
            "Epoch 273/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1013 - acc: 1.0000\n",
            "Epoch 274/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.1005 - acc: 1.0000\n",
            "Epoch 275/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0997 - acc: 1.0000\n",
            "Epoch 276/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0992 - acc: 1.0000\n",
            "Epoch 277/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0984 - acc: 1.0000\n",
            "Epoch 278/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0973 - acc: 1.0000\n",
            "Epoch 279/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0965 - acc: 1.0000\n",
            "Epoch 280/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0956 - acc: 1.0000\n",
            "Epoch 281/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0948 - acc: 1.0000\n",
            "Epoch 282/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0942 - acc: 1.0000\n",
            "Epoch 283/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0934 - acc: 1.0000\n",
            "Epoch 284/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0927 - acc: 1.0000\n",
            "Epoch 285/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0917 - acc: 1.0000\n",
            "Epoch 286/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0913 - acc: 1.0000\n",
            "Epoch 287/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0908 - acc: 1.0000\n",
            "Epoch 288/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0900 - acc: 1.0000\n",
            "Epoch 289/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0894 - acc: 1.0000\n",
            "Epoch 290/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0886 - acc: 1.0000\n",
            "Epoch 291/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0880 - acc: 1.0000\n",
            "Epoch 292/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0872 - acc: 1.0000\n",
            "Epoch 293/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0864 - acc: 1.0000\n",
            "Epoch 294/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0857 - acc: 1.0000\n",
            "Epoch 295/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0848 - acc: 1.0000\n",
            "Epoch 296/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0841 - acc: 1.0000\n",
            "Epoch 297/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0834 - acc: 1.0000\n",
            "Epoch 298/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0827 - acc: 1.0000\n",
            "Epoch 299/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0821 - acc: 1.0000\n",
            "Epoch 300/300\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.0815 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77DekOArjrV6",
        "colab_type": "code",
        "outputId": "ec7081df-974d-443d-d969-d9d3e413e4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history['loss'], label='loss')\n",
        "plt.show()\n",
        "plt.plot(history['acc'], 'r', label='accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}